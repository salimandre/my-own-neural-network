# My own neural network
> My own implementation of a dense neural network

## Installation & Usage

OS X & Linux:

```sh
pip3 install -r requirements.txt
python3 main.py
```

## Implemented features

* Layer: Dense
* Activation function: ReLu
* Weights initializer: Random Normal, Xavier, Glorot&amp;Bengio, He&amp;Zhang&amp;Ren&amp;Sun
* Optimizer: SGD, SGD with momentum, Nesterov AGD, RMSProp, AdaGrad, Adam
* Regularization: L2
* Drop out: No
* Batch normalization: No

## Dataset used

* The MNIST DATABASE of handwritten digits

<img src="mnist_illustration.png" width="30%">

## Results

A few motivating and useful examples of how your product can be used. Spice this up with code blocks and potentially more screenshots.

_For more examples and usage, please refer to the [Wiki][wiki]._
